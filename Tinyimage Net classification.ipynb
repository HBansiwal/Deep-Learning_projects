{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/ML/ImageNetModel/mini_train/n01443537_254.JPEG\n"
     ]
    }
   ],
   "source": [
    "path=[]\n",
    "#size_100 = (100,100)\n",
    "for f in os.listdir('D:/ML/ImageNetModel/mini_train'):\n",
    "    if f.endswith('.JPEG'):\n",
    "        #print(f)\n",
    "        path.append(\"D:/ML/ImageNetModel/mini_train/\"+f)\n",
    "        addr=shuffle(path)\n",
    "print(addr[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_creat(start,batchsize,new_size):\n",
    "    data=[]\n",
    "    label=[]\n",
    "    batch=addr[start:start+batchsize]\n",
    "    for i in range(batchsize):\n",
    "        img=cv2.imread(batch[i])\n",
    "        img.resize(new_size,new_size,3)#rgb=3\n",
    "        data.append(img)\n",
    "    for j in range(batchsize):\n",
    "        name=batch[j].split('/')[-1]\n",
    "        name=name.split('.')[-2]\n",
    "        name=name.split('_')[-2]\n",
    "        if(name=='n01443537'):\n",
    "            label.append([1,0,0,0,0,0,0,0,0,0])\n",
    "        elif(name=='n01629819'):\n",
    "            label.append([0,1,0,0,0,0,0,0,0,0])\n",
    "        elif(name=='n01641577'):\n",
    "            label.append([0,0,1,0,0,0,0,0,0,0])\n",
    "        elif(name=='n01698640'):\n",
    "            label.append([0,0,0,1,0,0,0,0,0,0])\n",
    "        elif(name=='n01644900'):\n",
    "            label.append([0,0,0,0,1,0,0,0,0,0])\n",
    "        elif(name=='n01742172'):\n",
    "            label.append([0,0,0,0,0,1,0,0,0,0])\n",
    "        elif(name=='n01768244'):\n",
    "            label.append([0,0,0,0,0,0,1,0,0,0])\n",
    "        elif(name=='n01770393'):\n",
    "            label.append([0,0,0,0,0,0,0,1,0,0])\n",
    "        elif(name=='n01774384'):\n",
    "            label.append([0,0,0,0,0,0,0,0,1,0])\n",
    "        \n",
    "        else:\n",
    "            label.append([0,0,0,0,0,0,0,0,0,1])\n",
    "    return data,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,label=batch_creat(0,100,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.shape(data))\n",
    "#print(np.shape(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/ML/ImageNetModel/mini_test/n01629819_299.JPEG\n"
     ]
    }
   ],
   "source": [
    "t_path=[]\n",
    "#size_100 = (100,100)\n",
    "for n in os.listdir('D:/ML/ImageNetModel/mini_test'):\n",
    "    if n.endswith('.JPEG'):\n",
    "        #print(f)\n",
    "        t_path.append(\"D:/ML/ImageNetModel/mini_test/\"+n)\n",
    "        t_addr=shuffle(t_path)\n",
    "print(t_addr[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_batch_creat(t_start,t_batchsize,t_new_size):\n",
    "    t_data=[]\n",
    "    t_label=[]\n",
    "    t_batch=t_addr[t_start:t_start+t_batchsize]\n",
    "    for i in range(t_batchsize):\n",
    "        img=cv2.imread(t_batch[i])\n",
    "        img.resize(t_new_size,t_new_size,3)#rgb=3\n",
    "        t_data.append(img)\n",
    "    for j in range(t_batchsize):\n",
    "        t_name=t_batch[j].split('/')[-1]\n",
    "        t_name=t_name.split('.')[-2]\n",
    "        t_name=t_name.split('_')[-2]\n",
    "        if(t_name=='n01443537'):\n",
    "            t_label.append([1,0,0,0,0,0,0,0,0,0])\n",
    "        elif(t_name=='n01629819'):\n",
    "            t_label.append([0,1,0,0,0,0,0,0,0,0])\n",
    "        elif(t_name=='n01641577'):\n",
    "            t_label.append([0,0,1,0,0,0,0,0,0,0])\n",
    "        elif(t_name=='n01698640'):\n",
    "            t_label.append([0,0,0,1,0,0,0,0,0,0])\n",
    "        elif(t_name=='n01644900'):\n",
    "            t_label.append([0,0,0,0,1,0,0,0,0,0])\n",
    "        elif(t_name=='n01742172'):\n",
    "            t_label.append([0,0,0,0,0,1,0,0,0,0])\n",
    "        elif(t_name=='n01768244'):\n",
    "            t_label.append([0,0,0,0,0,0,1,0,0,0])\n",
    "        elif(t_name=='n01770393'):\n",
    "            t_label.append([0,0,0,0,0,0,0,1,0,0])\n",
    "        elif(t_name=='n01774384'):\n",
    "            t_label.append([0,0,0,0,0,0,0,0,1,0])\n",
    "        \n",
    "        else:\n",
    "            t_label.append([0,0,0,0,0,0,0,0,0,1])\n",
    "        \n",
    "    return t_data,t_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_data, t_label=t_batch_creat(0,440,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(440, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(t_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/ML/ImageNetModel/mini_val/n01768244_84.JPEG\n"
     ]
    }
   ],
   "source": [
    "val_path=[]\n",
    "#size_100 = (100,100)\n",
    "for m in os.listdir('D:/ML/ImageNetModel/mini_val'):\n",
    "    if m.endswith('.JPEG'):\n",
    "        #print(f)\n",
    "        val_path.append(\"D:/ML/ImageNetModel/mini_val/\"+m)\n",
    "        val_addr=shuffle(val_path)\n",
    "print(val_addr[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_batch_creat(val_start,val_batchsize,val_new_size):\n",
    "    val_data=[]\n",
    "    val_label=[]\n",
    "    val_batch=val_addr[val_start:val_start+val_batchsize]\n",
    "    for i in range(val_batchsize):\n",
    "        img=cv2.imread(val_batch[i])\n",
    "        img.resize(val_new_size,val_new_size,3)#rgb=3\n",
    "        val_data.append(img)\n",
    "    for j in range(val_batchsize):\n",
    "        val_name=val_batch[j].split('/')[-1]\n",
    "        val_name=val_name.split('.')[-2]\n",
    "        val_name=val_name.split('_')[-2]\n",
    "        if(val_name=='n01443537'):\n",
    "            val_label.append([1,0,0,0,0,0,0,0,0,0])\n",
    "        elif(val_name=='n01629819'):\n",
    "            val_label.append([0,1,0,0,0,0,0,0,0,0])\n",
    "        elif(val_name=='n01641577'):\n",
    "            val_label.append([0,0,1,0,0,0,0,0,0,0])\n",
    "        elif(val_name=='n01698640'):\n",
    "            val_label.append([0,0,0,1,0,0,0,0,0,0])\n",
    "        elif(val_name=='n01644900'):\n",
    "            val_label.append([0,0,0,0,1,0,0,0,0,0])\n",
    "        elif(val_name=='n01742172'):\n",
    "            val_label.append([0,0,0,0,0,1,0,0,0,0])\n",
    "        elif(val_name=='n01768244'):\n",
    "            val_label.append([0,0,0,0,0,0,1,0,0,0])\n",
    "        elif(val_name=='n01770393'):\n",
    "            val_label.append([0,0,0,0,0,0,0,1,0,0])\n",
    "        elif(val_name=='n01774384'):\n",
    "            val_label.append([0,0,0,0,0,0,0,0,1,0])\n",
    "        \n",
    "        else:\n",
    "            val_label.append([0,0,0,0,0,0,0,0,0,1])\n",
    "    return val_data, val_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data,val_label=val_batch_creat(0,440,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(440, 10)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(val_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.shape(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.shape(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)#truncated, anything outside -0.2 and 0.2 has a probability of occurence 0\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)#constant array of 0.1 with whatever shape we give to it\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 64,64,3])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 3, 32])#5X5 filter 1 is the in channel (rbg) and there are 32 feature maps\n",
    "b_conv1 = bias_variable([32])#32 feature maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)#64X28X28X32. Here 64 is the batch size (assumed)\n",
    "h_pool1 = max_pool_2x2(h_conv1)#max pooling 64X14X14X32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])#64X32X32X64\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)#64X16X16X64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W_conv3 = weight_variable([5, 5, 64, 128])#128X64X64X128\n",
    "#b_conv3 = bias_variable([128])\n",
    "\n",
    "#h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "#h_pool3 = max_pool_2x2(h_conv3)#128X32X32X128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([16 * 16 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 16*16*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#W_fc2 = weight_variable([32 * 32 * 128, 2048])\n",
    "#b_fc2 = bias_variable([2048])\n",
    "\n",
    "#h_pool3_flat = tf.reshape(h_pool2, [-1, 32*32*128])\n",
    "#h_fc2 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)#probability of retaining the neurons. 0=all neurons dropped, 1 means no neuron is dropped\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024,10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_x, val_y=batch_creat(0,231,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.04\n",
      "53.8104248047\n",
      "step 1, training accuracy 0.08\n",
      "24.7446337891\n",
      "step 2, training accuracy 0.12\n",
      "17.2980822754\n",
      "step 3, training accuracy 0.16\n",
      "21.6696069336\n",
      "step 4, training accuracy 0.12\n",
      "19.4268579102\n",
      "validation accuracy  0.0977273\n",
      "step 0, training accuracy 0.16\n",
      "16.863404541\n",
      "step 1, training accuracy 0.12\n",
      "23.6051367187\n",
      "step 2, training accuracy 0.04\n",
      "22.5616162109\n",
      "step 3, training accuracy 0.04\n",
      "17.4154467773\n",
      "step 4, training accuracy 0.4\n",
      "12.249753418\n",
      "validation accuracy  0.125\n",
      "step 0, training accuracy 0.2\n",
      "14.1151013184\n",
      "step 1, training accuracy 0.16\n",
      "19.7015808105\n",
      "step 2, training accuracy 0.2\n",
      "13.1112426758\n",
      "step 3, training accuracy 0.24\n",
      "10.4631896973\n",
      "step 4, training accuracy 0.36\n",
      "9.96496276855\n",
      "validation accuracy  0.143182\n",
      "step 0, training accuracy 0.28\n",
      "8.45118896484\n",
      "step 1, training accuracy 0.36\n",
      "8.23129394531\n",
      "step 2, training accuracy 0.32\n",
      "8.24896179199\n",
      "step 3, training accuracy 0.2\n",
      "9.63566040039\n",
      "step 4, training accuracy 0.4\n",
      "8.64360778809\n",
      "validation accuracy  0.159091\n",
      "step 0, training accuracy 0.32\n",
      "6.99322570801\n",
      "step 1, training accuracy 0.44\n",
      "6.79879150391\n",
      "step 2, training accuracy 0.36\n",
      "6.57164123535\n",
      "step 3, training accuracy 0.2\n",
      "8.00532592773\n",
      "step 4, training accuracy 0.6\n",
      "4.79878509521\n",
      "validation accuracy  0.118182\n",
      "step 0, training accuracy 0.32\n",
      "4.9818359375\n",
      "step 1, training accuracy 0.28\n",
      "6.37063232422\n",
      "step 2, training accuracy 0.4\n",
      "6.20528930664\n",
      "step 3, training accuracy 0.52\n",
      "3.65134216309\n",
      "step 4, training accuracy 0.52\n",
      "2.79442504883\n",
      "validation accuracy  0.172727\n",
      "step 0, training accuracy 0.4\n",
      "3.33325469971\n",
      "step 1, training accuracy 0.44\n",
      "4.99932006836\n",
      "step 2, training accuracy 0.56\n",
      "4.77983215332\n",
      "step 3, training accuracy 0.48\n",
      "3.11626220703\n",
      "step 4, training accuracy 0.68\n",
      "1.82465591431\n",
      "validation accuracy  0.152273\n",
      "step 0, training accuracy 0.52\n",
      "2.10140533447\n",
      "step 1, training accuracy 0.64\n",
      "3.38791015625\n",
      "step 2, training accuracy 0.44\n",
      "2.70764190674\n",
      "step 3, training accuracy 0.6\n",
      "2.00087265015\n",
      "step 4, training accuracy 0.76\n",
      "1.57745162964\n",
      "validation accuracy  0.175\n",
      "step 0, training accuracy 0.52\n",
      "2.72533233643\n",
      "step 1, training accuracy 0.64\n",
      "2.78905090332\n",
      "step 2, training accuracy 0.68\n",
      "1.73764694214\n",
      "step 3, training accuracy 0.6\n",
      "1.60629165649\n",
      "step 4, training accuracy 0.68\n",
      "2.00950241089\n",
      "validation accuracy  0.206818\n",
      "step 0, training accuracy 0.6\n",
      "2.71916992187\n",
      "step 1, training accuracy 0.64\n",
      "1.62267578125\n",
      "step 2, training accuracy 0.6\n",
      "1.73161758423\n",
      "step 3, training accuracy 0.8\n",
      "1.13225517273\n",
      "step 4, training accuracy 0.84\n",
      "0.861279754639\n",
      "validation accuracy  0.209091\n",
      "test accuracy  0.145455\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:#declaring tf.Session as sess\n",
    "    sess.run(tf.global_variables_initializer())#initializing all the variables we used\n",
    "    for j in range(10):\n",
    "        for i in range(5):#number of iterations we are using\n",
    "            batchData,batchLabel = batch_creat(i*25, 25, 64) #taking the slice of input data\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batchData , y_: batchLabel, keep_prob: 1.0})#training accuracy calculated with no dropout probabilty\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "            cost=cross_entropy.eval(feed_dict={x:batchData , y_: batchLabel, keep_prob: 1.0}) \n",
    "            print(cost/100)\n",
    "            train_step.run(feed_dict={x: batchData, y_: batchLabel, keep_prob: 0.7})#dropout probabilty during training is kept 0.5\n",
    "        print('validation accuracy ', accuracy.eval(feed_dict={x: val_data, y_: val_label, keep_prob: 1.0}))#val accuracy\n",
    "    print('test accuracy ', accuracy.eval(feed_dict={x: t_data, y_: t_label, keep_prob: 1.0}))#test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
